{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a1d2847",
   "metadata": {},
   "source": [
    "# PART I: TITLE CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8cf8b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7c3419a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_maker(year):\n",
    "    \n",
    "    #list of all the port names in the files\n",
    "    port_names = [\n",
    "    \"Aigun\",\n",
    "    \"Harbin District\",\n",
    "    \"Hunchun & Lungchingtsun\",\n",
    "    \"Moukden\",\n",
    "    \"Antung\",\n",
    "    \"Dairen\",\n",
    "    \"Newchwang\",\n",
    "    \"Chinwangtao & Tientsin\",\n",
    "    \"Lungkow & Chefoo\",\n",
    "    \"Kiaochow\",\n",
    "    \"Chungking & Wanhsien\",\n",
    "    \"Ichang\",\n",
    "    \"Shasi\",\n",
    "    \"Changsha\",\n",
    "    \"Yochow\",\n",
    "    \"Hankow\",\n",
    "    \"Kiukiang\",\n",
    "    \"Wuhu\",\n",
    "    \"Nanking\",\n",
    "    \"Chinkiang\",\n",
    "    \"Shanghai\",\n",
    "    \"Soochow\",\n",
    "    \"Hangchow\",\n",
    "    \"Ningpo\",\n",
    "    \"Wenchow\",\n",
    "    \"Santuao\",\n",
    "    \"Foochow\",\n",
    "    \"Amoy\",\n",
    "    \"Swatow\",\n",
    "    \"Canton\",\n",
    "    \"Kowloon\",\n",
    "    \"Lappa\",\n",
    "    \"Kongmoon\",\n",
    "    \"Samshui\",\n",
    "    \"Wuchow\",\n",
    "    \"Nanning\",\n",
    "    \"Kiungchow\",\n",
    "    \"Pakhoi\",\n",
    "    \"Lungchow\",\n",
    "    \"Mengtsz\",\n",
    "    \"Szemao\",\n",
    "    \"Tengyueh\"\n",
    "]\n",
    "    #list of all possible table types in the files \n",
    "    all_table_types = ['Revenue','Shipping','Reports To The Customs','Values','Imports','Exports','Inland Transit' ,'Treasure','Passenger Traffic','Special']\n",
    "    \n",
    "    \n",
    "    folder_path = '/Users/hinaljajal/Downloads/' + str(year) + '/csv_and_txt'\n",
    "    \n",
    "    files_list = os.listdir(folder_path)\n",
    "    \n",
    "    #dataframe with names of all the files in the folder \n",
    "    df = pd.DataFrame(files_list,columns=['file_name'])\n",
    "    \n",
    "    df = df.sort_values(by=\"file_name\",ascending=True).reset_index().drop(columns=['index'])\n",
    "    \n",
    "    #get file type (csv or txt)\n",
    "    df[\"file_type\"] = df[\"file_name\"].str[-3:]\n",
    "    \n",
    "    df[\"file_name\"] = df[\"file_name\"].str[:-4]\n",
    "    \n",
    "    df[\"title_port\"] = np.nan\n",
    "    \n",
    "    df['title_port'] = df['title_port'].astype(object)\n",
    "\n",
    "    df[\"title_table_type\"] = np.nan\n",
    "    \n",
    "    df['title_table_type'] = df['title_table_type'].astype(object)\n",
    "    \n",
    "    #list that will contain the page numbers of all content pages\n",
    "    content_pages = []\n",
    "    \n",
    "    #loops through each txt file in the folder\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        if row['file_type'] == 'txt':\n",
    "            \n",
    "            file_path = '/Users/hinaljajal/Downloads/' + str(year) + '/txt/' + str(df.loc[index]['file_name'])+'.txt'\n",
    "            file = open(file_path, 'r')\n",
    "            Lines = file.readlines()\n",
    "            \n",
    "            #removes trailing characters from each line in the file\n",
    "            lines_list = [line.strip('\\n').strip('.').title() for line in Lines]\n",
    "            \n",
    "            #removes roman numerical characters from the beginning of all lines (this is for the port name titles)  \n",
    "            combinations_to_remove = [r\"I\\.?[-—─-]\", r\"Ii\\.?[-—─-]\",r\"Iii\\.?[-—─-]\",r\"Iv\\.?[-—─-]\",r\"V\\.?[-—─-]\",r\"Vi\\.?[-—─-]\",r\"Vii\\.?[-—─-]\",r\"Viii\\.?[-—─-]\",r\"Ix\\.?[-—─-]\"]\n",
    "            pattern = '|'.join(combinations_to_remove)\n",
    "            lines_list = [re.sub(pattern, \"\", line) for line in lines_list]\n",
    "            \n",
    "            #convert lists to sets\n",
    "            file_set = set(lines_list)\n",
    "            all_ports = set(port_names)\n",
    "            all_table_types = set(all_table_types)\n",
    "            \n",
    "            #check whether any port names occur in the file\n",
    "            common_elements_title_port = file_set.intersection(all_ports)\n",
    "\n",
    "            # Check if exactly one port name appears\n",
    "            if len(common_elements_title_port) == 1:\n",
    "                #if yes, this file is the cover/starting page for the port whose name appears  \n",
    "                df.at[index, 'title_port'] = list(common_elements_title_port)[0]\n",
    "\n",
    "            #check if any table type titles occur in the file \n",
    "            common_elements_title_table = file_set.intersection(all_table_types)\n",
    "            \n",
    "            #check if this file contains the word \"Contents\" once \n",
    "            if len(file_set.intersection({\"Contents\"})) == 1:\n",
    "                #if yes, this page is the contents page\n",
    "                content_pages.append(row['file_name'])\n",
    "            #else, check if exactly one table type name appears in this file \n",
    "            elif len(common_elements_title_table) == 1:\n",
    "                #if yes, this is the cover/starting page for that table type\n",
    "                df.at[index, 'title_table_type'] = list(common_elements_title_table)[0]\n",
    "            #else, check if exactly two different table type names appear in this file \n",
    "            elif len(common_elements_title_table) == 2:\n",
    "                #if yes, this is the cover/starting page for that table type\n",
    "                df.at[index, 'title_table_type'] = ','.join(common_elements_title_table)\n",
    "    \n",
    "    #forward fills the title column, ie. every page takes the value of the last valid/occuring title port name  \n",
    "    df['title_port'] = df['title_port'].fillna(method='ffill')\n",
    "    \n",
    "    df.loc[0]['title_port']=df.loc[1]['title_port']\n",
    "    \n",
    "    #within the same port name, every page takes the value of the last valid/occuring table type   \n",
    "    df[\"new_table_type\"] = df.groupby('title_port')['title_table_type'].fillna(method=\"ffill\")\n",
    "    \n",
    "    #marks all the content pages\n",
    "    df.loc[df['file_name'].isin(content_pages), 'new_table_type'] = \"Contents\"\n",
    "    \n",
    "    df = df.drop(columns={\"title_table_type\"})\n",
    "    df = df.rename(columns={\"new_table_type\":\"title_table_type\"})\n",
    "    \n",
    "    df[\"year\"] = year\n",
    "    df[\"file_path\"] = df[\"file_name\"] +\".\"+ df[\"file_type\"]\n",
    "    \n",
    "    #files that come before the table types are categorized as \"Notes\"   \n",
    "    df = df.fillna(\"Notes\")\n",
    "    \n",
    "    df[\"title\"] = df[\"title_port\"]+ \"-\" +df[\"title_table_type\"]\n",
    "    \n",
    "    return df[[\"title_port\",\"title_table_type\",\"year\",\"file_path\",\"title\"]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "be511908",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1923 = title_maker(1923)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c486d9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1924 = title_maker(1924)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d852730c",
   "metadata": {},
   "source": [
    "# PART II: NLP ENTITIES CREATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73a1465",
   "metadata": {},
   "source": [
    "### II(a) Create entities for all csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "16b535ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def nlp_entities_maker_csv(year):\n",
    "    \n",
    "    folder_path_csv = '/Users/hinaljajal/Downloads/'+str(year)+'/csv'\n",
    "    files_list = os.listdir(folder_path_csv)\n",
    "    csv_files_df = pd.DataFrame(files_list,columns=['file_name'])\n",
    "    csv_files_df['nlp_entities'] = '[]'\n",
    "\n",
    "    for index, row in csv_files_df.iterrows():\n",
    "\n",
    "        df = pd.read_csv(folder_path_csv + '/' +str(row[\"file_name\"]))\n",
    "        \n",
    "        #method 1 works only on \"Special\" files that have names of specific goods \n",
    "        try:\n",
    "            df[\"DESCRIPTION OF GOODS.\"] = df[\"DESCRIPTION OF GOODS.\"].fillna(\"\").astype(str)\n",
    "            \n",
    "            # Convert the description of goods column into a single string\n",
    "            combined_text = ' '.join(df[\"DESCRIPTION OF GOODS.\"].tolist())\n",
    "\n",
    "            #NER model\n",
    "            nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "            doc = nlp(combined_text)\n",
    "\n",
    "            # Extract and filter named entities with the desired label ('WORK_OF_ART')\n",
    "            entities = [ent.text for ent in doc.ents if ent.label_ == 'WORK_OF_ART']\n",
    "\n",
    "            # Count the occurrences of each named entity\n",
    "            entities_counter = Counter(entities)\n",
    "\n",
    "            # Get the top 7 most important 'WORK_OF_ART' entities based on frequency\n",
    "            top_7_work_of_art_entities = entities_counter.most_common(7)\n",
    "\n",
    "            csv_files_df.at[index, 'nlp_entities'] = top_7_work_of_art_entities\n",
    "        \n",
    "        #method 2: works on rest of the csv files\n",
    "        except KeyError: \n",
    "            #converts all columns to string and concatenate their values into a single string\n",
    "            combined_text = ' '.join(df.astype(str).apply(lambda x: ' '.join(x), axis=1).tolist())\n",
    "\n",
    "            #includs the column names as potential named entities\n",
    "            column_names = list(df.columns)\n",
    "            combined_text += ' '.join(column_names)\n",
    "\n",
    "            #NER model\n",
    "            nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "            doc = nlp(combined_text)\n",
    "\n",
    "            #excludes entities with digits and entities containing 'NaN'\n",
    "            entities = [ent.text for ent in doc.ents if not any(char.isdigit() for char in ent.text) and 'nan' not in ent.text.lower()]\n",
    "\n",
    "            #counts the occurrences of each entity\n",
    "            entity_counts = Counter(entities)\n",
    "\n",
    "            #the 7 most common entities\n",
    "            most_common_entities = entity_counts.most_common(7)\n",
    "\n",
    "            csv_files_df.at[index, 'nlp_entities'] = most_common_entities\n",
    "            \n",
    "            csv_files_df = csv_files_df.rename(columns={\"file_name\":\"file_path\"})\n",
    "            \n",
    "    return csv_files_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c430640",
   "metadata": {},
   "source": [
    "### II(b) Create entities for all txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8637e01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_entities_maker_txt(year):\n",
    "\n",
    "    @Language.component(\"filter_non_english_entities\")\n",
    "    def filter_non_english_entities(doc):\n",
    "        filtered_ents = []\n",
    "        for ent in doc.ents:\n",
    "            if all(token.is_alpha for token in ent):\n",
    "                filtered_ents.append(ent)\n",
    "        doc.ents = filtered_ents\n",
    "        return doc\n",
    "    \n",
    "    #NER model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    #remove non-english entitites\n",
    "    nlp.add_pipe(\"filter_non_english_entities\", last=True)\n",
    "\n",
    "    folder_path_txt = '/Users/hinaljajal/Downloads/' + str(year) + '/txt'\n",
    "\n",
    "    files_list = os.listdir(folder_path_txt)\n",
    "    txt_files_df = pd.DataFrame(files_list,columns=['file_name'])\n",
    "    txt_files_df['nlp_entities'] = '[]'\n",
    "\n",
    "    for index, row in txt_files_df.iterrows():\n",
    "        file_path = folder_path_txt + '/' + str(row['file_name'])\n",
    "        # Read the content of the text file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        doc = nlp(text)\n",
    "\n",
    "        #excludes entities with digits and entities containing 'NaN'\n",
    "        entities = [ent.text for ent in doc.ents if not any(char.isdigit() for char in ent.text) and 'nan' not in ent.text.lower()]\n",
    "\n",
    "        #counts the occurrences of each entity\n",
    "        entity_counts = Counter(entities)\n",
    "\n",
    "        #the 7 most common entities\n",
    "        most_common_entities = entity_counts.most_common(7)\n",
    "\n",
    "        txt_files_df.at[index, 'nlp_entities'] = most_common_entities\n",
    "        \n",
    "        txt_files_df = txt_files_df.rename(columns={\"file_name\":\"file_path\"})\n",
    "        \n",
    "    return txt_files_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdc9f21",
   "metadata": {},
   "source": [
    "# FINAL FUNCTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4b87e827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# years = [1923, 1924, 1925, 1926, 1927, 1928]\n",
    "years = [1923]\n",
    "def spreadsheet_maker(year):\n",
    "#     for year in years: \n",
    "    csv_files_df = nlp_entities_maker_csv(year)\n",
    "    txt_files_df = nlp_entities_maker_txt(year)\n",
    "    nlp_entities_year = pd.concat([txt_files_df, csv_files_df], axis=0)\n",
    "    df_titles_year = title_maker(year)\n",
    "    all_cols_year = pd.merge(df_titles_year,nlp_entities_year,on=\"file_path\",how=\"left\")\n",
    "    return all_cols_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "862b4ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_port</th>\n",
       "      <th>title_table_type</th>\n",
       "      <th>year</th>\n",
       "      <th>file_path</th>\n",
       "      <th>title</th>\n",
       "      <th>nlp_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aigun</td>\n",
       "      <td>Notes</td>\n",
       "      <td>1923</td>\n",
       "      <td>43675355.txt</td>\n",
       "      <td>Aigun-Notes</td>\n",
       "      <td>[(December Quarter, 2), (China, 1), (one, 1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aigun</td>\n",
       "      <td>Notes</td>\n",
       "      <td>1923</td>\n",
       "      <td>43675356.txt</td>\n",
       "      <td>Aigun-Notes</td>\n",
       "      <td>[(Shanghai, 3), (Limited, 3), (HARVARD, 1), (L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aigun</td>\n",
       "      <td>Contents</td>\n",
       "      <td>1923</td>\n",
       "      <td>43675357.txt</td>\n",
       "      <td>Aigun-Contents</td>\n",
       "      <td>[(the Maritime Customs, 4), (the Maritime Cust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aigun</td>\n",
       "      <td>Notes</td>\n",
       "      <td>1923</td>\n",
       "      <td>43675358.txt</td>\n",
       "      <td>Aigun-Notes</td>\n",
       "      <td>[(Blagovestchensk, 3), (the year, 2), (Russian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aigun</td>\n",
       "      <td>Notes</td>\n",
       "      <td>1923</td>\n",
       "      <td>43675359.txt</td>\n",
       "      <td>Aigun-Notes</td>\n",
       "      <td>[(Aigun, 2), (Customs, 2), (the year, 2), (Oct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>Tengyueh</td>\n",
       "      <td>Inland Transit</td>\n",
       "      <td>1923</td>\n",
       "      <td>43676683.txt</td>\n",
       "      <td>Tengyueh-Inland Transit</td>\n",
       "      <td>[(Chinese Goods, 1), (Transit Certificate, 1),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090</th>\n",
       "      <td>Tengyueh</td>\n",
       "      <td>Inland Transit</td>\n",
       "      <td>1923</td>\n",
       "      <td>43676684.txt</td>\n",
       "      <td>Tengyueh-Inland Transit</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2091</th>\n",
       "      <td>Tengyueh</td>\n",
       "      <td>Inland Transit</td>\n",
       "      <td>1923</td>\n",
       "      <td>43676685.txt</td>\n",
       "      <td>Tengyueh-Inland Transit</td>\n",
       "      <td>[(個 位 二, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2092</th>\n",
       "      <td>Tengyueh</td>\n",
       "      <td>Inland Transit</td>\n",
       "      <td>1923</td>\n",
       "      <td>43676686.csv</td>\n",
       "      <td>Tengyueh-Inland Transit</td>\n",
       "      <td>[(INDIAN, 1), (JAPANESE, 1), (Marks, 1), (Roub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2093</th>\n",
       "      <td>Tengyueh</td>\n",
       "      <td>Inland Transit</td>\n",
       "      <td>1923</td>\n",
       "      <td>43676686.txt</td>\n",
       "      <td>Tengyueh-Inland Transit</td>\n",
       "      <td>[(the HAIKWAN TAEL, 1), (the Customs Revenue, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2094 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     title_port title_table_type  year     file_path                    title  \\\n",
       "0         Aigun            Notes  1923  43675355.txt              Aigun-Notes   \n",
       "1         Aigun            Notes  1923  43675356.txt              Aigun-Notes   \n",
       "2         Aigun         Contents  1923  43675357.txt           Aigun-Contents   \n",
       "3         Aigun            Notes  1923  43675358.txt              Aigun-Notes   \n",
       "4         Aigun            Notes  1923  43675359.txt              Aigun-Notes   \n",
       "...         ...              ...   ...           ...                      ...   \n",
       "2089   Tengyueh   Inland Transit  1923  43676683.txt  Tengyueh-Inland Transit   \n",
       "2090   Tengyueh   Inland Transit  1923  43676684.txt  Tengyueh-Inland Transit   \n",
       "2091   Tengyueh   Inland Transit  1923  43676685.txt  Tengyueh-Inland Transit   \n",
       "2092   Tengyueh   Inland Transit  1923  43676686.csv  Tengyueh-Inland Transit   \n",
       "2093   Tengyueh   Inland Transit  1923  43676686.txt  Tengyueh-Inland Transit   \n",
       "\n",
       "                                           nlp_entities  \n",
       "0     [(December Quarter, 2), (China, 1), (one, 1), ...  \n",
       "1     [(Shanghai, 3), (Limited, 3), (HARVARD, 1), (L...  \n",
       "2     [(the Maritime Customs, 4), (the Maritime Cust...  \n",
       "3     [(Blagovestchensk, 3), (the year, 2), (Russian...  \n",
       "4     [(Aigun, 2), (Customs, 2), (the year, 2), (Oct...  \n",
       "...                                                 ...  \n",
       "2089  [(Chinese Goods, 1), (Transit Certificate, 1),...  \n",
       "2090                                                 []  \n",
       "2091                                       [(個 位 二, 1)]  \n",
       "2092  [(INDIAN, 1), (JAPANESE, 1), (Marks, 1), (Roub...  \n",
       "2093  [(the HAIKWAN TAEL, 1), (the Customs Revenue, ...  \n",
       "\n",
       "[2094 rows x 6 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spreadsheet_maker(1923)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
